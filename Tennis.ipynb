{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementation\n",
    "\n",
    "When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(15,7)})\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128, lr=0.002):\n",
    "\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.dr1 = nn.Dropout(p=0.0)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.dr2 = nn.Dropout(p=0.0)\n",
    "\n",
    "        self.bn3 = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.dr3 = nn.Dropout(p=0.0)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(device)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        if len(state) == self.state_size:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        x = self.dr1(F.relu(self.fc1(self.bn1(state))))\n",
    "        x = self.dr2(F.relu(self.fc2(self.bn2(x))))\n",
    "        return self.dr3(F.tanh(self.fc3(self.bn3(x))))\n",
    "        \n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=128, fc2_units=128, lr=0.002):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.bns = nn.BatchNorm1d(fcs1_units)\n",
    "\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.dr1 = nn.Dropout(p=0.0)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=0)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(device)\n",
    "                \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        xs = self.bns(xs)\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = self.dr1(F.relu(self.fc2(x)))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.size = size\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.normal(0, 1, self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 2e-3         # learning rate of the actor \n",
    "LR_CRITIC = 2e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "ACTOR_UNITS_l1 = 128    # DNN layers units\n",
    "ACTOR_UNITS_l2 = 128\n",
    "CRITIC_UNITS_l1 = 128\n",
    "CRITIC_UNITS_l2 = 128\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agents():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed, ACTOR_UNITS_l1, ACTOR_UNITS_l2, lr=LR_ACTOR).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed, ACTOR_UNITS_l1, ACTOR_UNITS_l2, lr=LR_ACTOR).to(device)\n",
    "\n",
    "        # Critic Network\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed, CRITIC_UNITS_l1, CRITIC_UNITS_l2, lr=LR_CRITIC).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed, CRITIC_UNITS_l1, CRITIC_UNITS_l2, lr=LR_CRITIC).to(device)\n",
    "        \n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    " \n",
    "        for i in range(self.num_agents):\n",
    "            self.memory.add(states[i,:], actions[i,:], rewards[i], next_states[i,:], dones[i])\n",
    "\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states, add_noise=True):\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = np.zeros((self.num_agents, self.action_size))\n",
    "        \n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            for num, state in enumerate(states):\n",
    "                actions[num,:] = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            actions += self.noise.sample()\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- CRITIC ---------------------------- #\n",
    "         # 1) Calculate Y for target\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # 2) Process critic backpropagation\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.critic_local.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_local.optimizer.step()\n",
    "\n",
    "        # ---------------------------- ACTOR ---------------------------- #\n",
    "        # 3) Process actor backpropagation\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        \n",
    "        self.actor_local.optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_local.optimizer.step()\n",
    "\n",
    "        # 4) Update target networks parameters\n",
    "        self.update(self.critic_local, self.critic_target, TAU)\n",
    "        self.update(self.actor_local, self.actor_target, TAU) \n",
    "                    \n",
    "\n",
    "    def update(self, local_model, target_model, tau):\n",
    "\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def save_model(self):\n",
    "        torch.save(self.actor_local.state_dict(), 'trained_actor_model.pth')\n",
    "        torch.save(self.critic_local.state_dict(), 'trained_critic_model.pth')\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.actor_local.load_state_dict(torch.load('trained_actor_model.pth'))\n",
    "        self.actor_local.eval()\n",
    "        self.critic_local.load_state_dict(torch.load('trained_critic_model.pth'))\n",
    "        self.critic_local.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 10  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 20  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 30  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 40  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 50  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 60  --->  Avg Reward: 0.02  ---> Reward: 0.05\n",
      "Game 70  --->  Avg Reward: 0.02  ---> Reward: 0.05\n",
      "Game 80  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 90  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 100  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 110  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 120  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 130  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 140  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 150  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 160  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 170  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 180  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 190  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 200  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 210  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 220  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 230  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 240  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 250  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 260  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 270  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 280  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 290  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 300  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 310  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 320  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 330  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 340  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 350  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 360  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 370  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 380  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 390  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 400  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 410  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 420  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 430  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 440  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 450  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 460  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 470  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 480  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 490  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 500  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 510  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 520  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 530  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 540  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 550  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 560  --->  Avg Reward: 0.01  ---> Reward: 0.05\n",
      "Game 570  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 580  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 590  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 600  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 610  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 620  --->  Avg Reward: 0.02  ---> Reward: 0.0\n",
      "Game 630  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 640  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 650  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 660  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 670  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 680  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 690  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 700  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 710  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 720  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 730  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 740  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 750  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 760  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 770  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 780  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 790  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 800  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 810  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 820  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 830  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 840  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 850  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 860  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 870  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 880  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 890  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 900  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 910  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 920  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 930  --->  Avg Reward: 0.0  ---> Reward: 0.0\n",
      "Game 940  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 950  --->  Avg Reward: 0.01  ---> Reward: 0.1\n",
      "Game 960  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 970  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 980  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 990  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 1000  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 1010  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 1020  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 1030  --->  Avg Reward: 0.01  ---> Reward: 0.0\n",
      "Game 1040  --->  Avg Reward: 0.02  ---> Reward: 0.05\n",
      "Game 1050  --->  Avg Reward: 0.02  ---> Reward: 0.1\n",
      "Game 1060  --->  Avg Reward: 0.03  ---> Reward: 0.0\n",
      "Game 1070  --->  Avg Reward: 0.03  ---> Reward: 0.05\n",
      "Game 1080  --->  Avg Reward: 0.03  ---> Reward: 0.05\n",
      "Game 1090  --->  Avg Reward: 0.04  ---> Reward: 0.0\n",
      "Game 1100  --->  Avg Reward: 0.04  ---> Reward: 0.0\n",
      "Game 1110  --->  Avg Reward: 0.04  ---> Reward: 0.05\n",
      "Game 1120  --->  Avg Reward: 0.04  ---> Reward: 0.0\n",
      "Game 1130  --->  Avg Reward: 0.04  ---> Reward: 0.0\n",
      "Game 1140  --->  Avg Reward: 0.04  ---> Reward: 0.0\n",
      "Game 1150  --->  Avg Reward: 0.04  ---> Reward: 0.1\n",
      "Game 1160  --->  Avg Reward: 0.04  ---> Reward: 0.0\n",
      "Game 1170  --->  Avg Reward: 0.04  ---> Reward: 0.05\n",
      "Game 1180  --->  Avg Reward: 0.05  ---> Reward: 0.05\n",
      "Game 1190  --->  Avg Reward: 0.05  ---> Reward: 0.0\n",
      "Game 1200  --->  Avg Reward: 0.05  ---> Reward: 0.0\n",
      "Game 1210  --->  Avg Reward: 0.04  ---> Reward: 0.0\n",
      "Game 1220  --->  Avg Reward: 0.05  ---> Reward: 0.0\n",
      "Game 1230  --->  Avg Reward: 0.06  ---> Reward: 0.05\n",
      "Game 1240  --->  Avg Reward: 0.07  ---> Reward: 0.05\n",
      "Game 1250  --->  Avg Reward: 0.07  ---> Reward: 0.04\n",
      "Game 1260  --->  Avg Reward: 0.07  ---> Reward: 0.05\n",
      "Game 1270  --->  Avg Reward: 0.08  ---> Reward: 0.05\n",
      "Game 1280  --->  Avg Reward: 0.08  ---> Reward: 0.0\n",
      "Game 1290  --->  Avg Reward: 0.08  ---> Reward: 0.05\n",
      "Game 1300  --->  Avg Reward: 0.1  ---> Reward: 0.1\n",
      "Game 1310  --->  Avg Reward: 0.11  ---> Reward: 0.05\n",
      "Game 1320  --->  Avg Reward: 0.11  ---> Reward: 0.15\n",
      "Game 1330  --->  Avg Reward: 0.11  ---> Reward: 0.05\n",
      "Game 1340  --->  Avg Reward: 0.11  ---> Reward: 0.0\n",
      "Game 1350  --->  Avg Reward: 0.11  ---> Reward: 0.05\n",
      "Game 1360  --->  Avg Reward: 0.11  ---> Reward: 0.15\n",
      "Game 1370  --->  Avg Reward: 0.12  ---> Reward: 0.05\n",
      "Game 1380  --->  Avg Reward: 0.12  ---> Reward: 0.05\n",
      "Game 1390  --->  Avg Reward: 0.12  ---> Reward: 0.05\n",
      "Game 1400  --->  Avg Reward: 0.12  ---> Reward: 0.05\n",
      "Game 1410  --->  Avg Reward: 0.12  ---> Reward: 0.1\n",
      "Game 1420  --->  Avg Reward: 0.12  ---> Reward: 0.15\n",
      "Game 1430  --->  Avg Reward: 0.12  ---> Reward: 0.05\n",
      "Game 1440  --->  Avg Reward: 0.12  ---> Reward: 0.1\n",
      "Game 1450  --->  Avg Reward: 0.13  ---> Reward: 0.15\n",
      "Game 1460  --->  Avg Reward: 0.13  ---> Reward: 0.05\n",
      "Game 1470  --->  Avg Reward: 0.13  ---> Reward: 0.15\n",
      "Game 1480  --->  Avg Reward: 0.12  ---> Reward: 0.0\n",
      "Game 1490  --->  Avg Reward: 0.13  ---> Reward: 0.05\n",
      "Game 1500  --->  Avg Reward: 0.13  ---> Reward: 0.05\n",
      "Game 1510  --->  Avg Reward: 0.14  ---> Reward: 0.05\n",
      "Game 1520  --->  Avg Reward: 0.14  ---> Reward: 0.15\n",
      "Game 1530  --->  Avg Reward: 0.14  ---> Reward: 0.05\n",
      "Game 1540  --->  Avg Reward: 0.14  ---> Reward: 0.05\n",
      "Game 1550  --->  Avg Reward: 0.15  ---> Reward: 0.0\n",
      "Game 1560  --->  Avg Reward: 0.16  ---> Reward: 0.0\n",
      "Game 1570  --->  Avg Reward: 0.16  ---> Reward: 0.15\n",
      "Game 1580  --->  Avg Reward: 0.16  ---> Reward: 0.1\n",
      "Game 1590  --->  Avg Reward: 0.16  ---> Reward: 0.05\n",
      "Game 1600  --->  Avg Reward: 0.16  ---> Reward: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1610  --->  Avg Reward: 0.16  ---> Reward: 0.05\n",
      "Game 1620  --->  Avg Reward: 0.16  ---> Reward: 0.3\n",
      "Game 1630  --->  Avg Reward: 0.17  ---> Reward: 0.45\n",
      "Game 1640  --->  Avg Reward: 0.17  ---> Reward: 0.1\n",
      "Game 1650  --->  Avg Reward: 0.16  ---> Reward: 0.1\n",
      "Game 1660  --->  Avg Reward: 0.16  ---> Reward: 0.15\n",
      "Game 1670  --->  Avg Reward: 0.16  ---> Reward: 0.1\n",
      "Game 1680  --->  Avg Reward: 0.16  ---> Reward: 0.55\n",
      "Game 1690  --->  Avg Reward: 0.16  ---> Reward: 0.0\n",
      "Game 1700  --->  Avg Reward: 0.17  ---> Reward: 0.1\n",
      "Game 1710  --->  Avg Reward: 0.17  ---> Reward: 0.1\n",
      "Game 1720  --->  Avg Reward: 0.19  ---> Reward: 0.3\n",
      "Game 1730  --->  Avg Reward: 0.19  ---> Reward: 0.15\n",
      "Game 1740  --->  Avg Reward: 0.18  ---> Reward: 0.1\n",
      "Game 1750  --->  Avg Reward: 0.21  ---> Reward: 0.3\n",
      "Game 1760  --->  Avg Reward: 0.24  ---> Reward: 0.05\n",
      "Game 1770  --->  Avg Reward: 0.24  ---> Reward: 0.15\n",
      "Game 1780  --->  Avg Reward: 0.24  ---> Reward: 0.15\n",
      "Game 1790  --->  Avg Reward: 0.24  ---> Reward: 0.25\n",
      "Game 1800  --->  Avg Reward: 0.24  ---> Reward: 0.05\n",
      "Game 1810  --->  Avg Reward: 0.24  ---> Reward: 0.75\n",
      "Game 1820  --->  Avg Reward: 0.22  ---> Reward: 0.55\n",
      "Game 1830  --->  Avg Reward: 0.23  ---> Reward: 0.05\n",
      "Game 1840  --->  Avg Reward: 0.23  ---> Reward: 0.0\n",
      "Game 1850  --->  Avg Reward: 0.21  ---> Reward: 0.05\n",
      "Game 1860  --->  Avg Reward: 0.18  ---> Reward: 0.05\n",
      "Game 1870  --->  Avg Reward: 0.17  ---> Reward: 0.1\n",
      "Game 1880  --->  Avg Reward: 0.18  ---> Reward: 0.35\n",
      "Game 1890  --->  Avg Reward: 0.19  ---> Reward: 0.4\n",
      "Game 1900  --->  Avg Reward: 0.19  ---> Reward: 0.4\n",
      "Game 1910  --->  Avg Reward: 0.22  ---> Reward: 0.35\n",
      "Game 1920  --->  Avg Reward: 0.24  ---> Reward: 0.3\n",
      "Game 1930  --->  Avg Reward: 0.23  ---> Reward: 0.45\n",
      "Game 1940  --->  Avg Reward: 0.25  ---> Reward: 0.44\n",
      "Game 1950  --->  Avg Reward: 0.27  ---> Reward: 0.05\n",
      "Game 1960  --->  Avg Reward: 0.28  ---> Reward: 0.25\n",
      "Game 1970  --->  Avg Reward: 0.32  ---> Reward: 0.55\n",
      "Game 1980  --->  Avg Reward: 0.34  ---> Reward: 1.2\n",
      "Game 1990  --->  Avg Reward: 0.37  ---> Reward: 0.25\n",
      "Game 2000  --->  Avg Reward: 0.4  ---> Reward: 1.7\n",
      "Game 2010  --->  Avg Reward: 0.37  ---> Reward: 0.3\n",
      "Game 2020  --->  Avg Reward: 0.43  ---> Reward: 0.35\n",
      "Game 2030  --->  Avg Reward: 0.48  ---> Reward: 0.0\n",
      "Game 2040  --->  Avg Reward: 0.52  ---> Reward: 0.5\n",
      "Game 2050  --->  Avg Reward: 0.5  ---> Reward: 0.3\n",
      "Game 2060  --->  Avg Reward: 0.5  ---> Reward: 0.7\n"
     ]
    }
   ],
   "source": [
    "GAMES = 3000\n",
    "samples = 100\n",
    "\n",
    "scores_deque = deque(maxlen=samples)\n",
    "scores = []\n",
    "average = []\n",
    "\n",
    "agents = Agents(state_size=state_size, action_size=action_size, num_agents=num_agents, random_seed=0)\n",
    "\n",
    "for episode in range(1, GAMES+1):\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    \n",
    "    agents.reset_noise()\n",
    "    score = np.zeros(num_agents)\n",
    "    \n",
    "    while(True):\n",
    "              \n",
    "        actions = agents.act(states)                           # select an action\n",
    "        env_info = env.step(actions)[brain_name]               # send the action to the environment\n",
    "        next_states = env_info.vector_observations             # get the next states\n",
    "        rewards = env_info.rewards                             # get the rewards\n",
    "        dones = env_info.local_done                            # see if the episode has finished for any agent\n",
    "\n",
    "        agents.step(states, actions, rewards, next_states, dones)\n",
    "      \n",
    "        states = next_states\n",
    "        score += rewards\n",
    "        \n",
    "        if np.any(dones):\n",
    "            break\n",
    "        \n",
    "    scores_deque.append(np.max(score))\n",
    "    scores.append(np.max(score))\n",
    "    average.append(np.mean(scores_deque))\n",
    "      \n",
    "    if episode % 10 == 0 and episode != 0:\n",
    "        print('Game {0}  --->  Avg Reward: {1}  ---> Reward: {2}'.format(episode, round(abs(np.mean(scores_deque)), 2), round(abs(np.mean(score)), 2), end=\"\"))\n",
    "    if episode % 100 == 0 and episode != 0:\n",
    "        agents.save_model()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'avg_rewards': average}  \n",
    "  \n",
    "df_rewards = pd.DataFrame(data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=df_rewards[0:2060])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
